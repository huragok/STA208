{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "import sys\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "# Import the training file\n",
    "df = pd.read_csv('train_mat.csv') \n",
    "#n,d = df.shape\n",
    "X_train_raw = df.values[:, 1 :]\n",
    "y_train = df.values[:, 0]\n",
    "n, d = X_train_raw.shape\n",
    "\n",
    "# Evaluate the tf-idf transformation on the counts\n",
    "transformer = TfidfTransformer()\n",
    "X_train_tfidf = transformer.fit_transform(X_train_raw)\n",
    "\n",
    "# Import the testing file\n",
    "df = pd.read_csv('test_mat.csv') # Import the datafile\n",
    "X_test_raw = df.values[:, 1 :]\n",
    "y_test = df.values[:, 0]\n",
    "\n",
    "X_test_tfidf = transformer.fit_transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct classifier objects for cross validation\n",
    "cv = KFold(n, n_folds=5)\n",
    "C_range = np.logspace(-1, 7, 9) # Range of Parameter C, larger C means less regularization\n",
    "\n",
    "classifier_svc = SVC(kernel='linear') # The linear svm classifier on the raw counts\n",
    "classifier_logreg = linear_model.LogisticRegression() # The logistic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning case 0...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      " - The best parameters are {'C': 1.0} with a score of 0.9837655016910936\n",
      " - The score on the test set is 0.9881889763779528\n",
      " - Elapsed time is 494.67375515799995\n",
      "Tuning case 1...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed:  7.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - The best parameters are {'C': 10.0} with a score of 0.9871476888387825\n",
      " - The score on the test set is 0.9904386951631046\n",
      " - Elapsed time is 16.354865559000245\n",
      "Tuning case 2...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed:   14.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - The best parameters are {'C': 1000.0} with a score of 0.9862457722660654\n",
      " - The score on the test set is 0.9865016872890888\n",
      " - Elapsed time is 41.646580312999504\n",
      "Tuning case 3...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed:   40.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - The best parameters are {'C': 1000000.0} with a score of 0.9873731679819616\n",
      " - The score on the test set is 0.9910011248593926\n",
      " - Elapsed time is 2.2257968359999722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed:    1.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Tune the classifiers and evaluate their performances\n",
    "case = 0\n",
    "\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "\n",
    "for classifier in [classifier_svc, classifier_logreg]:\n",
    "    for X_train, X_test in [(X_train_raw, X_test_raw), (X_train_tfidf, X_test_tfidf)]:\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        print(\"Tuning case {0}...\".format(case))\n",
    "\n",
    "        grid = GridSearchCV(classifier, param_grid=dict(C=C_range), cv=cv, verbose=1, n_jobs=4)\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        print(\" - The best parameters are {0} with a score of {1}\".format(grid.best_params_, grid.best_score_))\n",
    "        print(\" - The score on the test set is {0}\".format(grid.score(X_test, y_test)))\n",
    "        \n",
    "        y_score = grid.best_estimator_.decision_function(X_test)\n",
    "        precision[case], recall[case], _ = precision_recall_curve(y_test, y_score)\n",
    "        fpr[case], tpr[case], _ = roc_curve(y_test, y_score)\n",
    "        \n",
    "        print(\" - Elapsed time is {0}\".format(timeit.default_timer() - start_time))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        case = case + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fef2b3316a0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib qt\n",
    "\n",
    "axis_font = {'size':'20'}\n",
    "mpl.rcParams['xtick.labelsize'] = 16\n",
    "mpl.rcParams['ytick.labelsize'] = 16\n",
    "\n",
    "labels = ['SVC-raw', 'SVC-tfidf', 'LogReg-raw', 'LogReg-tfidf']\n",
    "line_specs = ['b-', 'b--', 'r-', 'r--']\n",
    "\n",
    "# The PR curve\n",
    "plt.figure()\n",
    "for case in range(4):\n",
    "    plt.plot(recall[case], precision[case], line_specs[case], label=labels[case], linewidth=2)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.9, 1.05])\n",
    "plt.grid()\n",
    "plt.xlabel('Recall',  **axis_font)\n",
    "plt.ylabel('Precision',  **axis_font)\n",
    "plt.legend(loc=\"lower left\", prop={'size':16})\n",
    "\n",
    "# The ROC curve\n",
    "plt.figure()\n",
    "for case in range(4):\n",
    "    plt.plot(fpr[case], tpr[case], line_specs[case], label=labels[case], linewidth=2)\n",
    "\n",
    "#plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.9, 1.05])\n",
    "plt.grid()\n",
    "plt.xlabel('False Positive Rate',  **axis_font)\n",
    "plt.ylabel('True Positive Rate',  **axis_font)\n",
    "plt.legend(loc=\"lower right\", prop={'size':16})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
