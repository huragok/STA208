{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "import sys\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the dense matrix files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the training file\n",
    "df = pd.read_csv('densemats/train_mat.csv', header=None) \n",
    "#n,d = df.shape\n",
    "X_train_raw = df.values[:, 1 :]\n",
    "y_train = df.values[:, 0]\n",
    "n, d = X_train_raw.shape\n",
    "\n",
    "# Evaluate the tf-idf transformation on the counts\n",
    "transformer = TfidfTransformer()\n",
    "X_train_tfidf = transformer.fit_transform(X_train_raw)\n",
    "\n",
    "# Import the testing file\n",
    "df = pd.read_csv('densemats/test_mat.csv', header=None) # Import the datafile\n",
    "X_test_raw = df.values[:, 1 :]\n",
    "y_test = df.values[:, 0]\n",
    "\n",
    "X_test_tfidf = transformer.fit_transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the sparse matrix files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the training file\n",
    "df_train = pd.read_csv('spmats/train_Xsp.csv', header=None)\n",
    "df_test = pd.read_csv('spmats/test_Xsp.csv', header=None)\n",
    "d = max(df_train.values[:, 1].max(), df_test.values[:, 1].max()) + 1\n",
    "n_train, n_test = df_train.values[:, 0].max() + 1, df_test.values[:, 0].max() + 1\n",
    "\n",
    "X_train_raw = csr_matrix((df_train.values[:, 2], (df_train.values[:, 0], df_train.values[:, 1])), shape=(n_train, d), dtype=\"float64\")\n",
    "y_train = pd.read_csv('spmats/train_y.csv', header=None).values[:, 0] \n",
    "n = X_train_raw.shape[0]\n",
    "\n",
    "# Evaluate the tf-idf transformation on the counts\n",
    "transformer = TfidfTransformer()\n",
    "X_train_tfidf = transformer.fit_transform(X_train_raw)\n",
    "\n",
    "# Import the testing file\n",
    "X_test_raw = csr_matrix((df_test.values[:, 2], (df_test.values[:, 0], df_test.values[:, 1])), shape=(n_test, d), dtype=\"float64\")\n",
    "y_test = pd.read_csv('spmats/test_y.csv', header=None).values[:, 0] \n",
    "\n",
    "X_test_tfidf = transformer.fit_transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct classifier objects for cross validation\n",
    "cv = KFold(n, n_folds=5)\n",
    "C_range = np.logspace(-1, 7, 9) # Range of Parameter C, larger C means less regularization\n",
    "\n",
    "classifier_svc = SVC(kernel='linear') # The linear svm classifier on the raw counts\n",
    "classifier_logreg = linear_model.LogisticRegression() # The logistic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning case 0...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      " - The best parameters are {'C': 1.0} with a score of 0.9837691614066727\n",
      " - The score on the test set is 0.9881956155143339\n",
      " - Elapsed time is 4.391517632999239\n",
      "Tuning case 1...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed:    3.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - The best parameters are {'C': 10.0} with a score of 0.9871505861136158\n",
      " - The score on the test set is 0.9904440697020798\n",
      " - Elapsed time is 14.584976989001007\n",
      "Tuning case 2...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed:   13.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - The best parameters are {'C': 1000.0} with a score of 0.986248872858431\n",
      " - The score on the test set is 0.9865092748735245\n",
      " - Elapsed time is 3.1231523290007317\n",
      "Tuning case 3...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - The best parameters are {'C': 1000000.0} with a score of 0.9873760144274121\n",
      " - The score on the test set is 0.9910061832490163\n",
      " - Elapsed time is 1.9424515230002726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed:    1.8s finished\n"
     ]
    }
   ],
   "source": [
    "# Tune the classifiers and evaluate their performances\n",
    "case = 0\n",
    "\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "\n",
    "for classifier in [classifier_svc, classifier_logreg]:\n",
    "    for X_train, X_test in [(X_train_raw, X_test_raw), (X_train_tfidf, X_test_tfidf)]:\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        print(\"Tuning case {0}...\".format(case))\n",
    "\n",
    "        grid = GridSearchCV(classifier, param_grid=dict(C=C_range), cv=cv, verbose=1, n_jobs=4)\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        print(\" - The best parameters are {0} with a score of {1}\".format(grid.best_params_, grid.best_score_))\n",
    "        print(\" - The score on the test set is {0}\".format(grid.score(X_test, y_test)))\n",
    "        \n",
    "        y_score = grid.best_estimator_.decision_function(X_test)\n",
    "        precision[case], recall[case], _ = precision_recall_curve(y_test, y_score)\n",
    "        fpr[case], tpr[case], _ = roc_curve(y_test, y_score)\n",
    "        \n",
    "        print(\" - Elapsed time is {0}\".format(timeit.default_timer() - start_time))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        case = case + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd6140bd400>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib qt\n",
    "\n",
    "axis_font = {'size':'20'}\n",
    "mpl.rcParams['xtick.labelsize'] = 16\n",
    "mpl.rcParams['ytick.labelsize'] = 16\n",
    "\n",
    "labels = ['SVC-raw', 'SVC-tfidf', 'LogReg-raw', 'LogReg-tfidf']\n",
    "line_specs = ['b-', 'b--', 'r-', 'r--']\n",
    "\n",
    "# The PR curve\n",
    "plt.figure()\n",
    "for case in range(4):\n",
    "    plt.plot(recall[case], precision[case], line_specs[case], label=labels[case], linewidth=2)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.9, 1.05])\n",
    "plt.grid()\n",
    "plt.xlabel('Recall',  **axis_font)\n",
    "plt.ylabel('Precision',  **axis_font)\n",
    "plt.legend(loc=\"lower left\", prop={'size':16})\n",
    "\n",
    "# The ROC curve\n",
    "plt.figure()\n",
    "for case in range(4):\n",
    "    plt.plot(fpr[case], tpr[case], line_specs[case], label=labels[case], linewidth=2)\n",
    "\n",
    "#plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.9, 1.05])\n",
    "plt.grid()\n",
    "plt.xlabel('False Positive Rate',  **axis_font)\n",
    "plt.ylabel('True Positive Rate',  **axis_font)\n",
    "plt.legend(loc=\"lower right\", prop={'size':16})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
