%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Structured General Purpose Assignment
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Original author:
% Ted Pavlic (http://www.tedpavlic.com)
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing assignment content.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
% PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage{graphicx} % Required to insert images
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{amsmath, amsfonts, bm, physics}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}
\usepackage[toc,page]{appendix}
\usepackage{algorithm}
\usepackage{algpseudocode}

\lstset{
    %numbers=left,
    stepnumber=1,    
    firstnumber=1,
    numberfirstline=true,
    basicstyle=\ttfamily,
    keywordstyle=\color{blue}\ttfamily,
    stringstyle=\color{red}\ttfamily,
    commentstyle=\color{green}\ttfamily,
    breaklines=true,
}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in 

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle} % Top center header
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
% DOCUMENT STRUCTURE COMMANDS
% Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
  \nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
  \nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
  \nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
  \nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
  \stepcounter{homeworkProblemCounter} % Increase counter for number of
% problems
  \setcounter{homeworkSectionctr}{0}
  %\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the
% name of the problem
  \section{\arabic{homeworkProblemCounter}. #1} %
  % Make a section in the document with the
% custom problem count
  \enterProblemHeader{\homeworkProblemName} % Header and footer within the
% environment
}{
  \exitProblemHeader{\homeworkProblemName} % Header and footer after the
% environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
  \noindent\textbf{\emph{Answer: }}#1 % Just put a keyword Answer in
  % bold/italic at the beginning
}

%\newcommand{\homeworkSectionName}{}
%\newenvironment{homeworkSection}[1]{ % New environment for sections within
% homework problems, takes 1 argument - the name of the section
%  \renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the
% name of the section from the environment argument
%  \subsection{\homeworkSectionName} % Make a subsection with the custom name
% of the subsection
%  \enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header
% and footer within the environment
%}{
%  \enterProblemHeader{\homeworkProblemName} % Header and footer after the
% environment
%}

\newcounter{homeworkSectionctr}
\newenvironment{homeworkSection}{
  \medskip\noindent%         create a vertical offset to previous material
  \refstepcounter{homeworkSectionctr}% increment the environment's counter
  \textbf{(\alph{homeworkSectionctr})\ }% or \textbf,.
}

\newtheorem{theorem}{Theorem}[homeworkProblemCounter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{
  \begin{trivlist}
    \item[\hskip \labelsep {\bfseries #1}]}{
  \end{trivlist}
}
\newenvironment{definition}[1][Definition]{
  \begin{trivlist}
    \item[\hskip \labelsep {\bfseries #1}]}{
  \end{trivlist}
}

\newenvironment{example}[1][Example]{
  \begin{trivlist}
    \item[\hskip \labelsep {\bfseries #1}]}{
  \end{trivlist}
}
    
\newenvironment{remark}[1][Remark]{
  \begin{trivlist}
    \item[\hskip \labelsep {\bfseries #1}]}{
  \end{trivlist}
}

\newcommand{\qed}{
  \nobreak \ifvmode \relax \else
  \ifdim\lastskip<1.5em \hskip-\lastskip
  \hskip1.5em plus0em minus0.5em \fi \nobreak
  \vrule height0.75em width0.5em depth0.25em\fi
}

\lstset{
  frame=single,
  breaklines=true,
  postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{red}\hookrightarrow\space}}
}
   
%----------------------------------------------------------------------------------------
% NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Assignment\ \#3} % Assignment title
\newcommand{\hmwkDueDate}{Monday, April\ 25,\ 2016} % Due date
\newcommand{\hmwkClass}{STA\ 208} % Course/class
\newcommand{\hmwkClassTime}{MW 12:00 - 2:00 P.M.} % Class/lecture time
\newcommand{\hmwkClassInstructor}{Prof. James Sharpnack} % Teacher/lecturer
\newcommand{\hmwkAuthorName}{Wenhao Wu} % Your name

%----------------------------------------------------------------------------------------
% TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
  \vspace{2in}
  \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
  \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
  \vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
  \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

  \maketitle
  
  %----------------------------------------------------------------------------------------
  % TABLE OF CONTENTS
  %----------------------------------------------------------------------------------------
  
  %\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC
  
  \newpage
  \tableofcontents
  \newpage
  
  %----------------------------------------------------------------------------------------
  % PROBLEM 1
  %----------------------------------------------------------------------------------------
  \begin{homeworkProblem}
    This question is about your final project.
    
    \begin{homeworkSection}
      List the members of your group.
      \vspace{10pt}
      
      \problemAnswer{
        
      }
    \end{homeworkSection}
    
    \begin{homeworkSection}
      What is the title of your project?
      \vspace{10pt}
      
      \problemAnswer{
        
      }
      
    \end{homeworkSection}
    
    \begin{homeworkSection}
      Give a brief description of what you intend to do.
      \vspace{10pt}
      
      \problemAnswer{
        
      }
    \end{homeworkSection}
    
  \end{homeworkProblem}
  %\clearpage
  
  %----------------------------------------------------------------------------------------
  % PROBLEM 2
  %----------------------------------------------------------------------------------------
  \begin{homeworkProblem}
    Consider the following linear classification loss function:
    \begin{align}
      l(\bm{\beta};y_i,\mathbf{x}_i) = (1 - y_i\mathbf{x}_i^T\bm{\beta})_+^2
      \notag
    \end{align}
    where $z_+ = \max\{0, z\}$. Consider the modified support vector machine
    program,
    \begin{align}
      \min_{\bm{\beta}\in\mathbb{R}^p} \frac{1}{n}\sum_{i=1}^n
      l(\bm{\beta};y_i,\mathbf{x}_i) + \lambda \|\bm{\beta}\|_2^2.
      \label{eq:mod_svm}
    \end{align}
    \begin{homeworkSection}
       Compute the gradient of the objective in~\eqref{eq:mod_svm} and verify
       that it is continuous.
      \vspace{10pt}
      
      \problemAnswer{
        Note that $l(\bm{\beta};y_i,\mathbf{x}_i) =
        f(y_i\mathbf{x}_i^T\bm{\beta})$, where $f(x) = (1 - x)_+^2$. Since
        \begin{align}
          \dv{f}{x} = \left\{
            \begin{array}{ll}
              -2(1-x) & x < 1 \\
              0 & x \geq 1 \\
            \end{array}
          \right.
        \end{align}
        therefore $f(x)$ is smooth. Consequently, we have
        \begin{align}
          \dv{l}{\bm{\beta}} &= f'(y_i\mathbf{x}_i^T\bm{\beta})y_i\mathbf{x}_i
          \notag \\ 
          &= \left\{
            \begin{array}{ll}
              -2(1-y_i\mathbf{x}_i^T\bm{\beta})y_i\mathbf{x}_i &
              y_i\mathbf{x}_i^T\bm{\beta} < 1 \\
              0 & y_i\mathbf{x}_i^T\bm{\beta} \geq 1 \\
            \end{array}
          \right.
        \end{align}
        which is also continuous.
      }
    \end{homeworkSection}
    
    \begin{homeworkSection}
      Write the pseudocode for gradient descent with exact line search for this
      gradient (you do not need to write out the bisection algorithm for exact
      line search, only state what exact line search is.).
      \vspace{10pt}
      
      \problemAnswer{
        \begin{algorithm}
          \caption{Sequential Optimization and Grouping of MBMS based on
          Weighted Sum Efficiency}
          \label{alg:wse_greedy}
          \begin{algorithmic}[1]
            \State Initialize $\bm{\beta} = \bm{\beta}_0$. Define the objective
            function $l(\bm{\beta}) = (1/n)\sum_{i=1}^n
            l(\bm{\beta};y_i,\mathbf{x}_i) + \lambda \|\bm{\beta}\|_2^2.$ 
            \Repeat
              \State Evaluate the gradient descent direction $\Delta\bm{\beta} =
              -\sum_{i=1}^n (1/n)dl(\bm{\beta};y_i,\mathbf{x}_i)/d\bm{\beta} -
              2\lambda\bm{\beta}$
              \State Choose a step with exact line search, i.e. $\hat{t} =
              \arg\min_{t\geq 0} l(\bm{\beta} + t\Delta\bm{\beta})$
              \State Update $\bm{\beta}:=\bm{\beta} +
              \hat{t}\Delta\bm{\beta}$
            \Until {Stopping criterion is satisfied.}
          \end{algorithmic}
        \end{algorithm}
      }
    \end{homeworkSection}
      
    \begin{homeworkSection}
      Suppose that we make a transformation $\mathbf{z}_i = \Phi(\mathbf{x}_i)$
      and we use this as the design matrix. Rewrite the minimization
      for~\eqref{eq:mod_svm} with the generalized kernel trick (Hint: make the
      substitution $\bm{\beta} = \mathbf{Z}^T \bm{\gamma}$ and write it as a
      function of $\bm{\gamma}$ and define $\mathbf{K} = \mathbf{ZZ}^T$.).
      \vspace{10pt}
      
      \problemAnswer{
        Substitute $\mathbf{x}_i$ with $\mathbf{z}_i$ and $\bm{\beta} =
        \mathbf{Z}^T\bm{\gamma}$, we have
        \begin{align}
          l(\bm{\gamma};y_i, \mathbf{z}_i) = (1-y_i\mathbf{z}_i^T\bm{\beta})_+^2
          = (1-y_i\mathbf{z}_i^T\mathbf{Z}^T\bm{\gamma})_+^2 =
          (1-y_i\mathbf{k}_i^T\bm{\gamma})_+^2
        \end{align}
        where $\mathbf{k}_i$ is the $i$-th column of $\mathbf{K}$. On the other
        hand,
        \begin{align}
          \|\bm{\gamma}\|_2^2 = \bm{\gamma}^T\mathbf{Z}\mathbf{Z}^T\bm{\gamma} =
          \bm{\gamma}^T\mathbf{K}\bm{\gamma}
        \end{align}
        Consequently, \eqref{eq:mod_svm} can be rewritten as
        \begin{align}
          \min_{\bm{\gamma}\in\mathbb{R}^n} \frac{1}{n}\sum_{i=1}^n
          (1-y_i\mathbf{k}_i^T\bm{\gamma})_+^2   + \lambda \bm{\gamma}^T\mathbf{K}\bm{\gamma}.
          \label{eq:mod_svm_kernel}
        \end{align}
      }
    \end{homeworkSection}
    
  \end{homeworkProblem}
  
  %----------------------------------------------------------------------------------------
  % PROBLEM 3
  %----------------------------------------------------------------------------------------
  \begin{homeworkProblem}
    Consider the following way in which we encode logical statements. For
    $\mathbf{x} \in \{0, 1\}^p$, we will encode the ``and'' and ``or''
    statements: ($x_i > 0$ and $x_j > 0$) is the same as ($x_ix_j > 0$); ($x_i >
    0$ or $x_j > 0$) is the same as ($x_i + x_j > 0$).
      
    \begin{homeworkSection}
      Suppose that $y_i = 1$ if and only if ($x_1 = 1$ and $x_2 = 1$) or $x_3 =
      1$. Write a function $f(\mathbf{x})$ such that $f(\mathbf{x}) = 1$ if and
      only if $y_i$.
      \vspace{10pt}
      
      \problemAnswer{
       \begin{align}
        f(\mathbf{x}) = x_1 x_2 + x_3
       \end{align}
      }
    \end{homeworkSection}
    
    \begin{homeworkSection}
      Define an embedding of $\mathbf{x}$ such that the dataset has a separating
      hyperplane, and write an equation for the separating hyperplane as a
      function of $\mathbf{z}_i = \Phi(\mathbf{x}_i)$.
      \vspace{10pt}
      
      \problemAnswer{
        The embedding is defined as
        \begin{align}
          \mathbf{z} = \Phi(\mathbf{x}) = \left[
            \begin{array}{c}
              x_1x_2 \\
              x_3
            \end{array}
          \right]
        \end{align}
        and a separating hyperplane for $\mathbf{z}_i$ is simply $z_1 + z_2 =
        0.5$ }
    \end{homeworkSection}
      
    \begin{homeworkSection}
      From this embedding construct the kernel function $k(\mathbf{x}_i,
      \mathbf{x}_j)$.
      \vspace{10pt}
      
      \problemAnswer{
        The kernel is defined as
        \begin{align}
          k(\mathbf{x}_i, \mathbf{x}_j) =
          \Phi(\mathbf{x}_i)^T\Phi(\mathbf{x}_j) = x_{i,1}x_{i,2}x_{j,1}x_{j,2}
          + x_{i,3}x_{j,3}
        \end{align}
      }
    \end{homeworkSection}

    \begin{homeworkSection}
      Suppose that $y_i = 1$ if and only if ($x_1 = 1$ or $x_2 = 1$) and ($x_3 =
      1$ and $x_4 = 1$). Write a function $f(\mathbf{x})$ such that
      $f(\mathbf{x}) > 0$ if and only if $y_i$.
      \vspace{10pt}
      
      \problemAnswer{
        \begin{align}
          f(\mathbf{x}) = (x_1 + x_2)x_3x_4 = x_1x_3x_4 + x_2x_3x_4
        \end{align}
      }
    \end{homeworkSection}
    
    \begin{homeworkSection}
      Define an embedding of $\mathbf{x}$ such that this new dataset has a
      separating hyperplane, and write an equation for the separating hyperplane
      as a function of $\mathbf{z}_i = \Phi(\mathbf{x}_i)$.
      \vspace{10pt}
      
      \problemAnswer{
        The embedding is defined as
        \begin{align}
          \mathbf{z} = \Phi(\mathbf{x}) = \left[
            \begin{array}{c}
              x_1x_3x_4 \\
              x_2x_3x_4
            \end{array}
          \right]
        \end{align}
        and a separating hyperplane for $\mathbf{z}_i$ is simply $z_1 + z_2 =
        0.5$ }
    \end{homeworkSection}
    
    \begin{homeworkSection}
       From this embedding construct the kernel function $k(\mathbf{x}_i,
      \mathbf{x}_j)$.
      \vspace{10pt}
      
      \problemAnswer{
        The kernel is defined as
        \begin{align}
          k(\mathbf{x}_i, \mathbf{x}_j) =
          \Phi(\mathbf{x}_i)^T\Phi(\mathbf{x}_j) = x_{i,1}x_{i,3}x_{i,4}
          x_{j,1}x_{j,3}x_{j,4} + x_{i,2}x_{i,3}x_{i,4}
          x_{j,2}x_{j,3}x_{j,4}
        \end{align}
      }
    \end{homeworkSection}
    
    \begin{homeworkSection}
      Define the degree of a logical statement as the maximum number of
      variables involved in any monomial after we apply the substitutions
      defined above. Give an embedding and kernel that separates a dataset where
      $y_i = 1$ if and only if some logical statements of degree 3 is true
      \vspace{10pt}
      
      \problemAnswer{
        
      }
    \end{homeworkSection}
  \end{homeworkProblem}
  
  %----------------------------------------------------------------------------------------
  % PROBLEM 4
  %----------------------------------------------------------------------------------------
  \begin{homeworkProblem}
    Download the training and test datasets from the website for the Reuters
    data. The first column is the response variable and the rest are the counts
    of each term in the dictionary for each document.
    
    \begin{homeworkSection}
      Tune a linear SVM with the raw counts as the design matrix and tune with
      5-fold cross validation
      \vspace{10pt}
      
    \end{homeworkSection}
    
    \begin{homeworkSection}
      Apply the tf-idf transformation and then tune a linear SVM with this as
      the design matrix, again with 5-fold cross validation.
      \vspace{10pt}
      
    \end{homeworkSection}
    
    \begin{homeworkSection}
      Tune logistic regression to these and tune it with 5-fold cross
      validation.
      \vspace{10pt}
      
    \end{homeworkSection}
    
    \begin{homeworkSection}
      Construct scores for each of these methods for the test set (this should
      be $\mathbf{x}_i^T\bm{\beta})$, and construct the ROC and PR curves for
      them.
      \vspace{10pt}
      
      \problemAnswer{
        We select the range of tuning parameters to be $\log
        C=\mbox{range}(-1,7)$ (where larger $C$ means less regularization) for
        both SVC and logistic regression and select the best one with a 5-fold
        cross validation. The training and testing scores of the 4 classifiers
        are shown in Table.~\ref{table:comparison}. The ROC and PR curves are
        shown in Figure~\ref{fig:roc_pr}. It appears that all the classifiers
        perform pretty well and training with design matrix after tf-idf
        transformation is advantageous.
        
        \begin{table}[!t]
          \renewcommand{\arraystretch}{1.3}
          \caption{Comparison of different classifier-design matrix.}
          \label{table:comparison}
          \centering
          \begin{tabular}{c|ccc}
            \hline
            Classifier-Design Matrix & $\hat{C}$ & Training score &
            Test score
            \\
            \hline
            SVC-Raw & $1$ & 0.9838 & 0.9882 \\
            SVC-tf-idf & $10$ & 0.9871 & 0.9904 \\
            LogReg-Raw & $10^3$ & 0.9862 & 0.9865 \\
            LogReg-tf-idf & $10^6$ & 0.9874 & 0.9910 \\
            \hline
          \end{tabular}
        \end{table}
        
        \begin{figure}[htb]
          \centering
          \begin{minipage}[b]{0.45\columnwidth}
            \centering
            \centerline{\includegraphics[width=8cm]{./figs/roc.png}}
            \centerline{(a) ROC}\medskip
          \end{minipage}
          \hfill
          \begin{minipage}[b]{0.45\columnwidth}
            \centering
            \centerline{\includegraphics[width=8cm]{./figs/pr.png}}
            \centerline{(b) PR}\medskip
          \end{minipage}
          \caption{Model performances.}
          \label{fig:roc_pr}
        \end{figure}
        
        The source code for this simulation is as follows:
      }
\begin{lstlisting}[language=Python]
import pandas as pd
import numpy as np
import timeit
import sys

from sklearn import linear_model
from sklearn.svm import SVC
from sklearn.cross_validation import KFold
from sklearn.grid_search import GridSearchCV
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.metrics import precision_recall_curve, roc_curve

# Import the training file
df = pd.read_csv('train_mat.csv') 
#n,d = df.shape
X_train_raw = df.values[:, 1 :]
y_train = df.values[:, 0]
n, d = X_train_raw.shape

# Evaluate the tf-idf transformation on the counts
transformer = TfidfTransformer()
X_train_tfidf = transformer.fit_transform(X_train_raw)

# Import the testing file
df = pd.read_csv('test_mat.csv') # Import the datafile
X_test_raw = df.values[:, 1 :]
y_test = df.values[:, 0]

X_test_tfidf = transformer.fit_transform(X_test_raw)

# Construct classifier objects for cross validation
cv = KFold(n, n_folds=5)
C_range = np.logspace(-1, 7, 9) # Range of Parameter C, larger C means less regularization

classifier_svc = SVC(kernel='linear') # The linear svm classifier on the raw counts
classifier_logreg = linear_model.LogisticRegression() # The logistic classifier

# Tune the classifiers and evaluate their performances
case = 0

precision = dict()
recall = dict()
fpr = dict()
tpr = dict()

for classifier in [classifier_svc, classifier_logreg]:
    for X_train, X_test in [(X_train_raw, X_test_raw), (X_train_tfidf, X_test_tfidf)]:
        start_time = timeit.default_timer()

        print("Tuning case {0}...".format(case))

        grid = GridSearchCV(classifier, param_grid=dict(C=C_range), cv=cv, verbose=1, n_jobs=4)
        grid.fit(X_train, y_train)

        print(" - The best parameters are {0} with a score of {1}".format(grid.best_params_, grid.best_score_))
        print(" - The score on the test set is {0}".format(grid.score(X_test, y_test)))
        
        y_score = grid.best_estimator_.decision_function(X_test)
        precision[case], recall[case], _ = precision_recall_curve(y_test, y_score)
        fpr[case], tpr[case], _ = roc_curve(y_test, y_score)
        
        print(" - Elapsed time is {0}".format(timeit.default_timer() - start_time))
        sys.stdout.flush()
        
        case = case + 1
        
# Visualization
import matplotlib.pyplot as plt
import matplotlib as mpl
\%matplotlib qt

axis_font = {'size':'20'}
mpl.rcParams['xtick.labelsize'] = 16
mpl.rcParams['ytick.labelsize'] = 16

labels = ['SVC-raw', 'SVC-tfidf', 'LogReg-raw', 'LogReg-tfidf']
line_specs = ['b-', 'b--', 'r-', 'r--']

# The PR curve
plt.figure()
for case in range(4):
    plt.plot(recall[case], precision[case], line_specs[case], label=labels[case], linewidth=2)

plt.xlim([0.0, 1.0])
plt.ylim([0.9, 1.05])
plt.grid()
plt.xlabel('Recall',  **axis_font)
plt.ylabel('Precision',  **axis_font)
plt.legend(loc="lower left", prop={'size':16})

# The ROC curve
plt.figure()
for case in range(4):
    plt.plot(fpr[case], tpr[case], line_specs[case], label=labels[case], linewidth=2)

#plt.plot([0, 1], [0, 1], 'k--', linewidth=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.9, 1.05])
plt.grid()
plt.xlabel('False Positive Rate',  **axis_font)
plt.ylabel('True Positive Rate',  **axis_font)
plt.legend(loc="lower right", prop={'size':16})
\end{lstlisting}
    \end{homeworkSection}
    
  \end{homeworkProblem}
  %\newpage
  %\begin{appendices} 
  %\end{appendices}
  %\bibliographystyle{unsrt}
  %\bibliography{refs}
  
  %----------------------------------------------------------------------------------------

\end{document}