%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Structured General Purpose Assignment
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Original author:
% Ted Pavlic (http://www.tedpavlic.com)
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing assignment content.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
% PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage{graphicx} % Required to insert images
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{amsmath, amsfonts, bm, physics}
\usepackage{xcolor}
\usepackage{listings}
\usepackage[toc,page]{appendix}

\lstset{
    %numbers=left,
    stepnumber=1,    
    firstnumber=1,
    numberfirstline=true,
    basicstyle=\ttfamily,
    keywordstyle=\color{blue}\ttfamily,
    stringstyle=\color{red}\ttfamily,
    commentstyle=\color{green}\ttfamily,
    breaklines=true,
}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in 

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle} % Top center header
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
% DOCUMENT STRUCTURE COMMANDS
% Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
  \nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
  \nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
  \nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
  \nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
  \stepcounter{homeworkProblemCounter} % Increase counter for number of
% problems
  \setcounter{homeworkSectionctr}{0}
  %\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the
% name of the problem
  \section{\arabic{homeworkProblemCounter}. #1} %
  % Make a section in the document with the
% custom problem count
  \enterProblemHeader{\homeworkProblemName} % Header and footer within the
% environment
}{
  \exitProblemHeader{\homeworkProblemName} % Header and footer after the
% environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
  \noindent\textbf{\emph{Answer: }}#1 % Just put a keyword Answer in
  % bold/italic at the beginning
}

%\newcommand{\homeworkSectionName}{}
%\newenvironment{homeworkSection}[1]{ % New environment for sections within
% homework problems, takes 1 argument - the name of the section
%  \renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the
% name of the section from the environment argument
%  \subsection{\homeworkSectionName} % Make a subsection with the custom name
% of the subsection
%  \enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header
% and footer within the environment
%}{
%  \enterProblemHeader{\homeworkProblemName} % Header and footer after the
% environment
%}

\newcounter{homeworkSectionctr}
\newenvironment{homeworkSection}{
  \medskip\noindent%         create a vertical offset to previous material
  \refstepcounter{homeworkSectionctr}% increment the environment's counter
  \textbf{(\alph{homeworkSectionctr})\ }% or \textbf,.
}

\newtheorem{theorem}{Theorem}[homeworkProblemCounter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{
  \begin{trivlist}
    \item[\hskip \labelsep {\bfseries #1}]}{
  \end{trivlist}
}
\newenvironment{definition}[1][Definition]{
  \begin{trivlist}
    \item[\hskip \labelsep {\bfseries #1}]}{
  \end{trivlist}
}

\newenvironment{example}[1][Example]{
  \begin{trivlist}
    \item[\hskip \labelsep {\bfseries #1}]}{
  \end{trivlist}
}
    
\newenvironment{remark}[1][Remark]{
  \begin{trivlist}
    \item[\hskip \labelsep {\bfseries #1}]}{
  \end{trivlist}
}

\newcommand{\qed}{
  \nobreak \ifvmode \relax \else
  \ifdim\lastskip<1.5em \hskip-\lastskip
  \hskip1.5em plus0em minus0.5em \fi \nobreak
  \vrule height0.75em width0.5em depth0.25em\fi
}

\lstset{
  frame=single,
  breaklines=true,
  postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{red}\hookrightarrow\space}}
}
   
%----------------------------------------------------------------------------------------
% NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Assignment\ \#1} % Assignment title
\newcommand{\hmwkDueDate}{Monday, April\ 4,\ 2016} % Due date
\newcommand{\hmwkClass}{STA\ 208} % Course/class
\newcommand{\hmwkClassTime}{MW 12:00 - 2:00 P.M.} % Class/lecture time
\newcommand{\hmwkClassInstructor}{Prof. James Sharpnack} % Teacher/lecturer
\newcommand{\hmwkAuthorName}{Wenhao Wu} % Your name

%----------------------------------------------------------------------------------------
% TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
  \vspace{2in}
  \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
  \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
  \vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
  \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

  \maketitle
  
  %----------------------------------------------------------------------------------------
  % TABLE OF CONTENTS
  %----------------------------------------------------------------------------------------
  
  %\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC
  
  \newpage
  \tableofcontents
  \newpage
  
  %----------------------------------------------------------------------------------------
  % PROBLEM 1
  %----------------------------------------------------------------------------------------
  \begin{homeworkProblem}[(Learning paradyms)]
    Decribe the issues involved in the following learning problems using the
    terminology that we learned in the first lecture. Provide a sentence or two
    for each problem.
      
    \begin{homeworkSection}
      A `smart farm' has distributed sensors that detect moisture levels, and
      the farmers know what are the ideal moisture levels for each plant. They
      have many controls that adjust the irrigation system and they would like
      to know what settings produce the most ideal moisture levels.
      \vspace{10pt}
      
      \problemAnswer{
        This problem can be studied as a supervised learning
        problem, more specifically, a regression problem. Given a plant, with $p$
        controls to adjust the mirrigation system, the farmers can try $N$
        different settings represented by an $N$-by-$p$ matrix $\mathbf{X}$
        (predictors) and use the sensors to record the observed moisture levels
        $\mathbf{y}$ (response) and use these data to find a relationship
        between the moisture level and the irrigation system settings
        $\hat{y}(\mathbf{x})$ and then solve the inverse problem $\hat{y}(\mathbf{x}_{opt}) =
        y_{opt}$ where $y_{opt}$ is the known optimal moisture level.
      }
    \end{homeworkSection}
    
    \begin{homeworkSection}
      Astronomers are trying to map the structure of the universe in terms of
      how galaxies cluster and form topological structures that they call
      filaments.
      \vspace{10pt}
      
      \problemAnswer{
        This problem can be studied as an unsupervised learning problem. The
        spacial locations of many galaxies are treated as data, and the how
        galaxies cluster and form topological structures are closely related to
        the distributions of the data.}
    \end{homeworkSection}
      
    \begin{homeworkSection}
      An online ad company wants to determine which of many ads to show each
      user based on their browser cookies.
      \vspace{10pt}
      
      \problemAnswer{
        This problem can be studied as a semi-supervised learning problem.
        The predictors are the cookies information for each user and the response
        is the ads that should be shown to the corresponding user. The response
        can be collected with a ``survey'' on a subset of users asking what are
        the ads they are interested in and how interested they are. However, not
        all users will answer the survey and no user can provide a comprehensive
        list of all the ads they are interested in.}
    \end{homeworkSection}
    
    \begin{homeworkSection}
      NASA is mapping the strength of the gravitational field on the surface of
      Mars. They want you to help with determining its values in a grid of
      locations on the surface from remote sensing measurements.
      \vspace{10pt}
      
      \problemAnswer{
        This problem can be studied as a supervised learning, regression
        problem. First NASA needs to measure the strength of the
        gravitational fields (responses) at $n$ locations (predictors) on the
        mars suface. With this data one can find a relationship between the 
        strength of the gravitational fields and the coordinate of a
        certain location and make predictions on new locations.}
    \end{homeworkSection}
  \end{homeworkProblem}
  %\clearpage
  
  %----------------------------------------------------------------------------------------
  % PROBLEM 2
  %----------------------------------------------------------------------------------------
  \begin{homeworkProblem}[(Bayes rule)]
    Consider the classification setting with features
    $\mathbf{x}\in\mathbb{R}^p$ and response $y \in \{0, 1\}$. Suppose that we
    know the joint distribution of $P(x, y)$, and the conditional
    distributions $P(y|x)$, $P(x|y)$ (an unlikely setting, but bear with me).
      
    \begin{homeworkSection}
      Under the Hamming loss, what is the true risk of a classifier $\hat{y}:
      \mathbb{R}^p \rightarrow \{0, 1\}$? Write it in terms of conditional
      distributions.
      \vspace{10pt}
      
      \problemAnswer{
        \begin{align}
          R(\hat{y}) &= \mathbb{E}_{\mathbf{x}, y}[1(\hat{y}(\mathbf{x})\not=y)]
          \notag
          \\
          &=
          \mathbb{E}_{\mathbf{x}}[\mathbb{E}_{y|\mathbf{x}}
          [1(\hat{y}(\mathbf{x})\not=y)]] \notag \\
          &= \mathbb{E}_{\mathbf{x}}[P(\hat{y}(\mathbf{x})\not=y|\mathbf{x})]
          \notag \\
          &= \int_{\hat{y}(\mathbf{x})=1}P(0|\mathbf{x})p(\mathbf{x})d\mathbf{x}
          + \int_{\hat{y}(\mathbf{x})=0}P(1|\mathbf{x})p(\mathbf{x})d\mathbf{x}
        \end{align}
        where $1(\cdot)$ is the indicator function.
      }
    \end{homeworkSection}
    
    \begin{homeworkSection}
      What is the Bayes rule, i.e. the classifier that minimizes the true risk?
      \vspace{10pt}
      
      \problemAnswer{
        We can rewrite the true risk into
        \begin{align}
          R(\hat{y}) &= \int_{\hat{y}(\mathbf{x})=1}[P(0|\mathbf{x}) -
          P(1|\mathbf{x})]p(\mathbf{x})d\mathbf{x} + \int
          P(1|\mathbf{x})p(\mathbf{x})d\mathbf{x}
        \end{align}
        Since the second term on the RHS does not depend on $\hat{y}$, to minimize
        $R(\hat{y})$, the Bayes rule is
        \begin{align}
          \hat{y}(\mathbf{x}) = \left\{
            \begin{array}{ll}
              1 & \mbox{if } P(0|\mathbf{x}) < P(1|\mathbf{x}), \\
              0 & \mbox{otherwise.}
            \end{array}
          \right.
        \end{align}
      }
      
    \end{homeworkSection}
      
    \begin{homeworkSection}
      Write in one sentence, what is the Bayes rule, as if you needed to
      describe what the Bayes risk was to someone in an elevator before you
      reached the lobby.
      \vspace{10pt}
      
      \problemAnswer{
        Bayes rule is the predictor for a supervised learning problem that
        minimizes the true risk, which is the risk taken expectation over the
        joint distribution of the predictors and responses $P(\mathbf{x}, y)$}
    \end{homeworkSection}
    
    \begin{homeworkSection}
      Prove that the Bayes risk is $1 - P(y = y^*(\mathbf{x})|\mathbf{x})$ where
      $y^*$ is the Bayes rule.
      \vspace{10pt}
      
      \problemAnswer{
        The Bayes risk given $\mathbf{x}$ is
        \begin{align}
          R(y^*(\mathbf{x})) =
          \mathbb{E}_{y|\mathbf{x}}[1(y^*(\mathbf{x}) \not= y)]] =
          P(y^*(\mathbf{x})\not=y|\mathbf{x}) = 1 -
          P(y^*(\mathbf{x})=y|\mathbf{x})
        \end{align}
      }
    \end{homeworkSection}
  \end{homeworkProblem}
  
  %----------------------------------------------------------------------------------------
  % PROBLEM 3
  %----------------------------------------------------------------------------------------
  \begin{homeworkProblem}[(Linear Regression)]
    Suppose that we are in the regression setting, $\mathbf{x}_i \in
    \mathbb{R}^p , y_i \in \mathbb{R}$ are $n$ pairs drawn iid, let $\mathbf{y}
    = (y_1,\ldots,y_n)$ and $\mathbf{X}^T = (\mathbf{x}_1 ,\ldots, \mathbf{x}_n
    )$, and consider the linear regression estimator
    \begin{align}
      \hat{\bm{\beta}}:= \arg\min_{\bm{\beta}\in\mathbb{R}^p} \|\mathbf{y} -
      \mathbf{X}\bm{\beta}\|_2^2.
      \label{eq:linear_regression}
    \end{align}
      
    \begin{homeworkSection}
      When is the solution to this program unique? In this case, what is the
      unique minimizer $\hat{\bm{\beta}}$?
      \vspace{10pt}
      
      \problemAnswer{
        The stationary condition to minimize the square-error loss function is
        \begin{align}
          \pdv{l(\hat{\mathbf{y}},\mathbf{y})}{\bm{\beta}} &=
          2\mathbf{X}^T(\mathbf{X}\bm{\beta} - \mathbf{y}) = \mathbf{0}
          \label{eq:stationary}
        \end{align}
        i.e. $\mathbf{X}^T\mathbf{X}\bm{\beta} = \mathbf{X}^T\mathbf{y}$. The
        solution to this program is unique iff $\mathbf{X}$ has a
        full colummn rank of $p$, or positive definite equivalently, when the
        unique solution t~\eqref{eq:stationary} is
        \begin{align}
          \hat{\bm{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
        \end{align} 
        Since the Hessian matrix $2\mathbf{X}^T\mathbf{X}$ is positive definite
        in this case, $\hat{\bm{\beta}}$ indeed minnimizes the square loss
        function.
        
        On the other hand, if $\mathbf{X}$ does not have full column rank, then
        there exists $\Delta\bm{\beta}\not= \mathbf{0}$ such that
        $\mathbf{X}\Delta\bm{\beta} = \mathbf{0}$. Consequently, if
        $\hat{\bm{\beta}}$ is a solution that minimizes~\eqref{eq:stationary},
        then $\hat{\bm{\beta}} + \Delta\bm{\beta} \not= \hat{\bm{\beta}}$
        results in exactly the same square loss, therefore the solution can not
        be unique.}
    \end{homeworkSection}
    
    \begin{homeworkSection}
      Give an equation that the minimizers satisfy regardless of uniqueness?
      \vspace{10pt}
      
      \problemAnswer{
        \eqref{eq:stationary} is the 1st-order necessary condition that the
        minimizers satisfy regardless of uniqueness.
      }
    \end{homeworkSection}
      
    \begin{homeworkSection}
      Given a solution to~\eqref{eq:linear_regression}, give a reasonable
      prediction rule $\hat{y}: \mathbb{R}^p \rightarrow \mathbb{R}$.
      \vspace{10pt}
      
      \problemAnswer{
        Assuming the solution to~\eqref{eq:linear_regression}
        $\hat{\bm{\beta}}$, a reasonable prediction rule is simply
        \begin{align}
          \hat{y}(\mathbf{x}) = \mathbf{x}^T\hat{\bm{\beta}}
        \end{align}
      }
    \end{homeworkSection}
    
    \begin{homeworkSection}
      Suppose that $\mathbb{E}[y_i | \mathbf{x}_i ] = \mathbf{x}_i^T\bm{\beta}$
      for $i = 1,\ldots, n + 1$. For a new random draw $(\mathbf{x}_{n+1},
      y_{n+1})$, then what is the bias of $\hat{y}(\mathbf{x}_{n+1})$, i.e.
      $\mathbb{E}[\hat{y}(\mathbf{x}_{n+1}) - y_{n+1}]$?
      \vspace{10pt}
      
      \problemAnswer{
        \begin{subequations}
          \begin{align}
            \mathbb{E}[\hat{y}(\mathbf{x}_{n+1}) - y_{n+1}] &=
            \mathbb{E}[\hat{y}(\mathbf{x}_{n+1})] - \mathbb{E}[y_{n+1}] \\
            &= \mathbb{E}[\mathbf{x}_{n+1}^T\hat{\beta}] -
            \mathbf{x}_{n+1}^T\bm{\beta} \\
            &=
            \mathbf{x}_{n+1}^T(\mathbf{X}^T\mathbf{X})^{-1}
            \mathbf{X}^T\mathbb{E}[\mathbf{y}] -
            \mathbf{x}_{n+1}^T\bm{\beta} \\
            &= \mathbf{x}_{n+1}^T(\mathbf{X}^T\mathbf{X})^{-1}
            \mathbf{X}^T(\mathbf{X}\bm{\beta}) -
            \mathbf{x}_{n+1}^T\bm{\beta} \\
            &= 0
          \end{align}
        \end{subequations}
      }
    \end{homeworkSection}
  \end{homeworkProblem}
  
  %----------------------------------------------------------------------------------------
  % PROBLEM 4
  %----------------------------------------------------------------------------------------
  \begin{homeworkProblem}[(Simulation and ridge regression.)]
    \begin{homeworkSection}
      Simulate $\{\mathbf{x}_i\} ^n_{i=1} \subset \mathbb{R}^p$ with $p = 12$
      and $n = 200$, iid normal with mean $\mathbf{0}$ and variance
      $\mathbf{\Sigma}$ such that
      \begin{align}
        \Sigma_{j,k}=\rho^{|j-k|},\;j,k = 1,\ldots, p.
      \end{align}
      for $0 < \rho < 1$. Draw $\bm{\beta} \in \mathbb{R}^p$ such that
      $\beta_j$ are iid normal with mean 0 and variance 1, and $y_i$
      independently normal with mean $\mathbf{x}_i^T\bm{\beta}$ and variance
      1. Print out your code (not the output), which should consist of functions
      for generating these objects.
      \vspace{10pt}
      
      \problemAnswer{}
\begin{lstlisting}[language=Python]
import numpy as np
from scipy.linalg import toeplitz

p = 12
n = 200
rho = 0.5

X = np.random.multivariate_normal(np.zeros(p, dtype="float64"),\
                                  toeplitz(rho ** np.arange(p)),\
                                  n)
beta = np.random.normal(0, 1, p)
y = np.dot(X, beta) + np.random.normal(0, 1, n)
\end{lstlisting}
    \end{homeworkSection}
    
    \begin{homeworkSection}
      Derive an analytical expression for the solution to ridge regression,
      \begin{align}
        \hat{\bm{\beta}}:= \arg\min_{\bm{\beta}\in\mathbb{R}^p} \|\mathbf{y} -
        \mathbf{X}\bm{\beta}\|_2^2 + \lambda\|\bm{\beta}\|_2^2,
      \label{eq:ridge_regression}
      \end{align}
      as a function of $\mathbf{X}, \mathbf{y}, λ$.
      \vspace{10pt}
      
      \problemAnswer{
        The stationary condition for~\eqref{eq:ridge_regression} is
        \begin{align}
          \pdv{(\|\mathbf{y} - \mathbf{X}\bm{\beta}\|_2^2 +
          \lambda\|\bm{\beta}\|_2^2)}{\bm{\beta}} &= 2(\mathbf{X}^T\mathbf{X} +
          \lambda\mathbf{I})\bm{\beta} - 2\mathbf{X}^T\mathbf{y} = \mathbf{0}
        \end{align}
        When $\lambda>0$, $(\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I})$ is
        always positive-definite. Therefore the solution to ridge regression is
        \begin{align}
          \hat{\bm{\beta}} = (\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I})^{-1}
          \mathbf{X}^T\mathbf{y}
        \end{align}
      }
    \end{homeworkSection}
      
    \begin{homeworkSection}
      Provide code for solving ridge regression. Use any linear solver you like.
      \vspace{10pt}
      
      \problemAnswer{}
\begin{lstlisting}[language=Python]
alpha = 0.5
C = np.dot(X.T, X) + alpha * np.eye(p)
beta_est = np.linalg.solve(C, np.dot(X.T, y))
\end{lstlisting}
    \end{homeworkSection}
    
    \begin{homeworkSection}
      Set $\rho = 0.5$. Simulate the bias of $\hat{\bm{\beta}}$,
      $\|\mathbb{E}\hat{\bm{\beta}} - \bm{\beta}\|_2^2$, the variance,
      $\mathbb{E}\|\hat{\bm{\beta}} - \mathbb{E}\hat{\bm{\beta}}\|_2^2$, and the
      mean square error, $\mathbb{E}\|\hat{\bm{\beta}} - \bm{\beta}\|_2^2$, for
      many values of $\lambda$. Be sure that you see instances of overfitting
      and underfitting and can clearly see the point where $\lambda$ is optimal.
      Plot these curves as functions of $\lambda$ and include your code.
      \vspace{10pt}
      
      \problemAnswer{
        Since $\mathbf{y} = \mathbf{X}\bm{\beta} + \mathbf{v}$ where
        $\mathbf{v}\sim\mathcal{N}(0, 1)$. Given a set of predictors
        $\mathbf{X}$, we have
        \begin{align}
          \mbox{bias}_{{\mathbf{X}}} &=
          \mathbb{E}_{\hat{\bm{\beta}}}\left[\|\mathbb{E}_{\mathbf{v}}[\hat{\bm{\beta}]}
          - \bm{\beta}\|_2^2\right]\notag \\
          &=
          \mathbb{E}_{\bm{\beta}}\left[
          \|\mathbb{E}_{\mathbf{v}}[(\mathbf{X}^T\mathbf{X} +
          \lambda\mathbf{I})^{-1}\mathbf{X}^T(\mathbf{X}\bm{\beta} +
          \mathbf{v})] - \bm{\beta}\|_2^2\right]\notag \\
          &=
          \mathbb{E}_{\bm{\beta}}
          \left[\bm{\beta}^T\mathbf{A}^T\mathbf{A}\bm{\beta}\right]\notag \\
          &= \|\mathbf{A}\|_F^2
        \end{align}
        where $\mathbf{A} = -\lambda(\mathbf{X}^T\mathbf{X} +
        \lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{X}$.
        Similarly, we have
        \begin{align}
          \mbox{var}_{\mathbf{X}} & =
          \mathbb{E}_{\bm{\beta}}\mathbb{E}_{\mathbf{v}}\left[\|\hat{\bm{\beta}}
          - \mathbb{E}_{\mathbf{v}}[\hat{\bm{\beta}}]\|_2^2\right]\notag \\
          &= \mathbb{E}_{\mathbf{v}}\left[\|(\mathbf{X}^T\mathbf{X} +
          \lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{v}\|_2^2\right]\notag \\
          &= \mathbb{E}_{\mathbf{v}}
          \left[\mathbf{v}^T\mathbf{B}^T\mathbf{B}\mathbf{v}\right]\notag
          \\
          &= \|\mathbf{B}\|_F^2
        \end{align}
        where $\mathbf{B} = (\mathbf{X}^T\mathbf{X} +
        \lambda\mathbf{I})^{-1}\mathbf{X}^T$. The mean square error given
        $\mathbf{X}$ can be simply evaluated as
        \begin{align}
          \mbox{mse}_{\mathbf{X}} & =
          \mathbb{E}_{\bm{\beta}}
          \left[\mathbb{E}_{\mathbf{v}}\|\hat{\bm{\beta}} -
          \bm{\beta}\|_2^2\right] \notag\\
          & =
          \mathbb{E}_{\bm{\beta}}
          \left[\mathbb{E}_{\mathbf{v}}\|(\hat{\bm{\beta}} -
          \mathbb{E}_{\mathbf{v}}[\hat{\bm{\beta}}]) +
          (\mathbb{E}_{\mathbf{v}}[\hat{\bm{\beta}}] - \bm{\beta})
          \|_2^2\right] \notag\\
          &= \mbox{bias}_{{\mathbf{X}}} + \mbox{var}_{{\mathbf{X}}} +
          2\mathbb{E}_{\bm{\beta}}
          \left[\mathbb{E}_{\mathbf{v}}\left[(\hat{\bm{\beta}} -
          \mathbb{E}_{\mathbf{v}}[\hat{\bm{\beta}}])^T (\mathbb{E}_{\mathbf{v}}[\hat{\bm{\beta}}] - \bm{\beta})\right]
          \right] \notag \\
          &= \mbox{bias}_{{\mathbf{X}}} + \mbox{var}_{{\mathbf{X}}}
        \end{align}
        As a result, the total bias, variance and mean-square error averaged
        over $\mathbf{X}$ can be evaluated with a Monte-Carlo simulation by
        generating $n_{\mathbf{X}}$ instances of $\mathbf{X}$ and take the
        empirical means:
        \begin{align}
          \mbox{bias}_{emp} = \frac{1}{n_{\mathbf{X}}} \sum_{i =
          1}^{n_{\mathbf{X}}} \mbox{bias}_{\mathbf{X}_i},\;
          \mbox{var}_{emp} = \frac{1}{n_{\mathbf{X}}} \sum_{i =
          1}^{n_{\mathbf{X}}} \mbox{var}_{\mathbf{X}_i}
        \end{align}
        and $\mbox{mse}_{emp} = \mbox{bias}_{emp} + \mbox{var}_{emp}$. The
        results are shown in Fig.~\ref{fig:tradeoff}. As $\lambda$ grows, the
        bias increases while the variance decreases. The minimum MSE is achieved
        at $\lambda = 1$, which corresponds to the maximization of the posterior
        distribution of $\bm{\beta}$~\cite[Sec.3.3.1]{bishop2006pattern}. The
        code is listed as follows:
        
        \begin{figure}[h]
          \centering
          \includegraphics[width=3.5in]{figs/tradeoff.png}
          \caption{Bias, variance ane MSE for ridge regression.}
          \label{fig:tradeoff}
        \end{figure}
      }
    \end{homeworkSection}
  \end{homeworkProblem}
\begin{lstlisting}[language=Python]
def get_error_cond_X(X, alpha):
    n, p = X.shape
    
    XX = np.dot(X.T, X)
    C = XX + alpha * np.eye(p)
    bias = np.linalg.norm(np.linalg.solve(C, XX) - np.eye(p)) ** 2
    var = np.linalg.norm(np.linalg.solve(C, X.T)) ** 2
    mse = bias + var

    return np.array((bias, var, mse))

def get_error(alpha, p = 12, n = 200, rho = 0.5, n_X = 1000, n_montecarlo = None):
    
    n_alpha = len(alpha)
    error = np.empty([n_alpha, n_X, 3], dtype="float64")
    X = np.random.multivariate_normal(np.zeros(p, dtype="float64"),\
                                  toeplitz(rho ** np.arange(p)),\
                                  (n_X, n))
    if n_montecarlo is None:
        for i_alpha in range(n_alpha):
            for i_X in range(n_X):
                error[i_alpha, i_X] = get_error_cond_X(X[i_X], alpha[i_alpha])
    else:
        for i_alpha in range(n_alpha):
            for i_X in range(n_X):
                n_beta, nrun = n_montecarlo
                error[i_alpha, i_X] = get_error_cond_X_montecarlo(X[i_X],\
                alpha[i_alpha], n_beta, nrun)
        
    return error.mean(axis=1)
    
alpha = np.linspace(0.0, 2.0, 11)
errors = get_error(alpha)

import matplotlib.pyplot as plt
%matplotlib qt

axis_font = {'size':'20'}

fig, axs = plt.subplots(3, 1, sharex=True)

axs[0].plot(alpha, errors[:, 0], "+-", lw=2, markersize=10, markeredgewidth=2)
axs[0].set_ylabel("Bias", **axis_font)
axs[0].grid(True)
axs[1].plot(alpha, errors[:, 1], "+-", lw=2, markersize=10, markeredgewidth=2)
axs[1].set_ylabel("Var", **axis_font)
axs[1].grid(True)
axs[2].plot(alpha, errors[:, 2], "+-", lw=2, markersize=10, markeredgewidth=2)
axs[2].set_ylabel("MSE", **axis_font)
axs[2].set_xlabel("$\lambda$", **axis_font)
axs[2].grid(True)
\end{lstlisting} 
  
  %----------------------------------------------------------------------------------------
  % PROBLEM 5
  %----------------------------------------------------------------------------------------
  \begin{homeworkProblem}[(Airfoil)]
    Download the airfoil dataset, which is linked in the homework section of the
    course site. We will focus on predicting the scaled sound pressure, which is
    the 6th row.
    \begin{homeworkSection}
      Set aside a test set at random.
      \vspace{10pt}
      
      \problemAnswer{
      
      }
    \end{homeworkSection}
    
    \begin{homeworkSection}
      Form the coefficients for ordinary least squares with the training set.
      Write a function with a new $\mathbf{x}$ and $\hat{\bm{\beta}}$ as
      arguments and returns the prediction. Use any linear solver/Cholesky
      decomposition you like.
      \vspace{10pt}
      
      \problemAnswer{
        
      }
    \end{homeworkSection}
      
    \begin{homeworkSection}
      Write a function that takes a new $\mathbf{x}, k$, and the training data,
      and outputs the k-nearest neighbor prediction with Euclidean distance.
      \vspace{10pt}
      
      \problemAnswer{
        
      }
    \end{homeworkSection}
    
    \begin{homeworkSection}
      Write a function that takes a new $\mathbf{x}$, a bandwidth parameter, and
      the training data, and outputs the kernel prediction with boxcar kernel
      and Euclidean distance.
      \vspace{10pt}
      
      \problemAnswer{
        
      }
    \end{homeworkSection}
    
    \begin{homeworkSection}
      Evaluate your methods on the test set, calculating the test error
      (empirical risk on the test set). Vary the tuning parameters and plot the
      test error as a function of the tuning parameters.
      \vspace{10pt}
      
      \problemAnswer{
        
      }
    \end{homeworkSection}
    
    \begin{homeworkSection}
      Is the best test error a good estimate of the true risk for these methods?
      Why/why not? What can be done to estimate the true risk?
      \vspace{10pt}
      
      \problemAnswer{
        
      }
    \end{homeworkSection}
  \end{homeworkProblem}
  
  %\newpage
  %\begin{appendices} 
  %\end{appendices}
  \bibliographystyle{unsrt}
  \bibliography{refs}
  
  %----------------------------------------------------------------------------------------

\end{document}